# Data Processing Boilerplate #

Template for building data processing systems using AWS cloud services and data store applications (such as ElasticSearch, Apache Druid, Presto, SQL databases). The specific business logic have been removed from the original code, but the structure maintains and would be useful for similar applications. 

The project is structured to be customizable but was originally created to be compatible with a system designed as follows:

1. Users generated data is collected via AWS Kinesis Firehose and dumped to S3 as files <10 MBs in size.

2. Packet's put event is used to trigger the data processing pipeline hosted within Kubernetes cluster. The data is then validated, standarized, and further enhanced/transformed depending on the data stream.

3. The processed data is then dumped to: 

    a) S3 as compressed parquets for long term storage and made available for further processing by Spark or AWS Athena

    b) ElasticSearch stack for visualizations, exploratory analysis, and monitoring 

    c) Apache Druid for low latency OLAP analysis and reporting

The project also contains various supportive functions orchestrated using Apache Airflow. These functionalities can be broadly broken down into three main areas:

1. maintenance the above mentioned applications to ensure a robust system for reliable data processing and services (eg. anomoly detection, database cleanups, health checks and notificaitons)

2. optimization of data handling and storage to reduce cost and boost query performances (eg. data compression, shard/partition management, data pre-aggregation and caching)

3. large scale data analysis and transformations for additional use cases 

## Project Structure ##

The *pipelines* directory maintains event based pipelines for low latency data processing. Specifically, the pipeline aims to collected raw data generated by users in near real time, validate/process/transform the data, and load the output for analysis and long term storage in multiple data store locations in less than few seconds. 

The *orchestrator* directory maintains time based jobs for large scale data processing using Apache Airflow as the orchestrator. Sepcifically, utilizing platforms and services such as AWS Athena/Presto, Apache Spark clusters, and Apach Druid clusters to process large quantities of data on a set time interval. These jobs also include various maintanence and notification functions to ensure the entire data processing system is functioning as expected.

The *common* directory contains various utilities code that integrates different platforms together and improve the speed of developing additional functions. 

The *static* directory contains templates for configs for applications managed by engineering team that are not services provided by cloud platforms

The *Dockerfile* builds a generic image for all the functions, but should be split/broken down as needed per use case to reduce image size. 

## Next Steps ##

Future iterations of the system would include additional integration to leverage AWS Fargate to reduce costs and improvements on dev-ops areas (eg. CI/CD, more tests, etc.)